{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3079, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience it's more convenient to build the model with a log-softmax output using nn.LogSoftmax or F.log_softmax (documentation). Then you can get the actual probabilites by taking the exponential torch.exp(output). With a log-softmax output, you want to use the negative log likelihood loss, nn.NLLLoss (documentation).\n",
    "\n",
    "Exercise: Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3097, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our log-probabilities\n",
    "logps = model(images)\n",
    "# Calculate the loss with the logps and the labels\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1467,  0.0764],\n",
      "        [ 0.0172, -0.7530]], requires_grad=True)\n",
      "tensor([[1.3149e+00, 5.8403e-03],\n",
      "        [2.9566e-04, 5.6699e-01]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x00000277BC9A9E80>\n",
      "tensor(0.4720, grad_fn=<MeanBackward0>)\n",
      "None\n",
      "tensor([[ 0.5734,  0.0382],\n",
      "        [ 0.0086, -0.3765]])\n",
      "None\n",
      "tensor([[ 0.5734,  0.0382],\n",
      "        [ 0.0086, -0.3765]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)\n",
    "y = x**2\n",
    "print(y)\n",
    "'''grad_fn shows the function that generated this variable'''\n",
    "print(y.grad_fn)\n",
    "'''The autograd module keeps track of these operations and knows how to calculate the gradient for each one. \n",
    "In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. \n",
    "Let's reduce the tensor y to a scalar value, the mean.'''\n",
    "z = y.mean()\n",
    "print(z)\n",
    "'''You can check the gradients for x and y but they are empty currently.'''\n",
    "print(x.grad)\n",
    "\n",
    "'''To calculate the gradients, you need to run the .backward method on a Variable, z for example. \n",
    "This will calculate the gradient for z with respect to x\n",
    "'''\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0024, -0.0024, -0.0024,  ..., -0.0024, -0.0024, -0.0024],\n",
      "        [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
      "        ...,\n",
      "        [-0.0046, -0.0046, -0.0046,  ..., -0.0046, -0.0046, -0.0046],\n",
      "        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010],\n",
      "        [-0.0007, -0.0007, -0.0007,  ..., -0.0007, -0.0007, -0.0007]])\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model(images)\n",
    "loss = criterion(logps, labels)\n",
    "\n",
    "'''When we create a network with PyTorch, all of the parameters are initialized with requires_grad = True. \n",
    "This means that when we calculate the loss and call loss.backward(), the gradients for the parameters are calculated. \n",
    "These gradients are used to update the weights with gradient descent. Below you can see an example of calculating the gradients\n",
    "using a backwards pass.` '''\n",
    "\n",
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network!\n",
    "There's one last piece we need to start training, an optimizer that we'll use to update the weights with the gradients. We get these from PyTorch's optim package. For example we can use stochastic gradient descent with optim.SGD. You can see how to define an optimizer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a forward pass through the network\n",
    "Use the network output to calculate the loss\n",
    "Perform a backward pass through the network with loss.backward() to calculate the gradients\n",
    "Take a step with the optimizer to update the weights\n",
    "\n",
    "\n",
    "optimizer.zero_grad(). When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0252, -0.0044, -0.0256,  ...,  0.0301, -0.0100,  0.0144],\n",
      "        [ 0.0172, -0.0344, -0.0058,  ..., -0.0160,  0.0116,  0.0259],\n",
      "        [ 0.0145, -0.0055, -0.0349,  ..., -0.0242,  0.0089,  0.0080],\n",
      "        ...,\n",
      "        [ 0.0214,  0.0269,  0.0245,  ..., -0.0228,  0.0116, -0.0026],\n",
      "        [-0.0084, -0.0120, -0.0143,  ...,  0.0347, -0.0077,  0.0128],\n",
      "        [-0.0295,  0.0066, -0.0133,  ...,  0.0287, -0.0120,  0.0297]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-0.0052, -0.0052, -0.0052,  ..., -0.0052, -0.0052, -0.0052],\n",
      "        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006],\n",
      "        [-0.0016, -0.0016, -0.0016,  ..., -0.0016, -0.0016, -0.0016],\n",
      "        ...,\n",
      "        [-0.0060, -0.0060, -0.0060,  ..., -0.0060, -0.0060, -0.0060],\n",
      "        [-0.0023, -0.0023, -0.0023,  ..., -0.0023, -0.0023, -0.0023],\n",
      "        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.914447036887537\n",
      "Training loss: 0.8376263892218503\n",
      "Training loss: 0.5216996668974982\n",
      "Training loss: 0.4276656572148998\n",
      "Training loss: 0.38337779103883546\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.9605e-03, 2.1605e-06, 1.6611e-03, 8.6057e-05, 7.6604e-03, 3.7065e-04,\n",
      "         6.8483e-05, 1.6366e-01, 4.7700e-03, 8.1777e-01]])\n"
     ]
    }
   ],
   "source": [
    "#With the network trained, we can check out it's predictions.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
